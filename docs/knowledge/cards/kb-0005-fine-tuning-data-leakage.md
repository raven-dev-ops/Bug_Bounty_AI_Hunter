# Fine-tuning data leakage risk

## Metadata
- ID: kb-0005
- Type: card
- Status: draft
- Tags: ai-security, fine-tuning, privacy
- Source: TRANSCRIPT_01.md
- Date: 2026-01-14

## Summary
Fine-tuning on private data can cause memorization or exposure paths. Treat fine-tuned models as sensitive data stores.

## Relevance to the project
Guides review of training data handling and model exposure controls.

## Safe notes
- Limit fine-tuning data to the minimum necessary.
- Document training data provenance and access controls.
- Avoid placing secrets in training data or prompts.
- Use access controls and monitoring around model endpoints.

## References
- knowledge/sources/TRANSCRIPT_01.md (approx 4:29-7:02)
